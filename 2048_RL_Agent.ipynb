{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Agent for 2048 Game\n",
    "\n",
    "This notebook implements a Deep Q-Learning (DQN) agent to play the 2048 game. The agent learns to maximize its score by making intelligent moves based on the current game state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers, models, optimizers\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implement 2048 Game Environment\n",
    "\n",
    "First, we need to create a game environment that the agent can interact with. This is an adaptation of the existing 2048 game that's been optimized for RL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game2048Env:\n",
    "    \"\"\"2048 game environment adapted for reinforcement learning\"\"\"\n",
    "    \n",
    "    # Action mappings\n",
    "    ACTIONS = {\n",
    "        0: 'left',\n",
    "        1: 'right',\n",
    "        2: 'up',\n",
    "        3: 'down'\n",
    "    }\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the game environment\"\"\"\n",
    "        self.size = 4\n",
    "        self.grid = None\n",
    "        self.score = 0\n",
    "        self.done = False\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the game to initial state\"\"\"\n",
    "        self.grid = np.zeros((self.size, self.size), dtype=int)\n",
    "        self.score = 0\n",
    "        self.done = False\n",
    "        \n",
    "        # Add two initial tiles\n",
    "        self._add_random_tile()\n",
    "        self._add_random_tile()\n",
    "        \n",
    "        return self._get_state()\n",
    "    \n",
    "    def _add_random_tile(self):\n",
    "        \"\"\"Add a random tile (2 or 4) to an empty cell\"\"\"\n",
    "        if not self._has_empty_cells():\n",
    "            return False\n",
    "        \n",
    "        # Find empty cells\n",
    "        empty_cells = [(i, j) for i in range(self.size) for j in range(self.size) if self.grid[i, j] == 0]\n",
    "        \n",
    "        # Choose a random empty cell\n",
    "        i, j = random.choice(empty_cells)\n",
    "        \n",
    "        # Add a 2 (90% chance) or 4 (10% chance)\n",
    "        self.grid[i, j] = 2 if random.random() < 0.9 else 4\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def _has_empty_cells(self):\n",
    "        \"\"\"Check if there are any empty cells\"\"\"\n",
    "        return 0 in self.grid\n",
    "    \n",
    "    def _compress_row(self, row):\n",
    "        \"\"\"Move all non-zero elements to the left\"\"\"\n",
    "        # Remove zeros\n",
    "        new_row = np.array([x for x in row if x != 0])\n",
    "        # Add zeros at the end\n",
    "        new_row = np.append(new_row, np.zeros(self.size - len(new_row), dtype=int))\n",
    "        return new_row\n",
    "    \n",
    "    def _merge_row(self, row):\n",
    "        \"\"\"Merge tiles of the same value in a row\"\"\"\n",
    "        score_increase = 0\n",
    "        \n",
    "        # Iterate through the list from left to right\n",
    "        i = 0\n",
    "        while i < self.size - 1:\n",
    "            # If current and next element are the same and not zero\n",
    "            if row[i] == row[i + 1] and row[i] != 0:\n",
    "                row[i] *= 2\n",
    "                score_increase += row[i]\n",
    "                row[i + 1] = 0\n",
    "                i += 2\n",
    "            else:\n",
    "                i += 1\n",
    "                \n",
    "        return row, score_increase\n",
    "    \n",
    "    def _move_left(self):\n",
    "        \"\"\"Move all tiles to the left and merge if possible\"\"\"\n",
    "        score_increase = 0\n",
    "        changed = False\n",
    "        \n",
    "        for i in range(self.size):\n",
    "            original_row = self.grid[i, :].copy()\n",
    "            \n",
    "            # Compress (move non-zero elements to the left)\n",
    "            row = self._compress_row(original_row)\n",
    "            \n",
    "            # Merge\n",
    "            row, score = self._merge_row(row)\n",
    "            score_increase += score\n",
    "            \n",
    "            # Compress again after merging\n",
    "            row = self._compress_row(row)\n",
    "            \n",
    "            # Update grid\n",
    "            if not np.array_equal(original_row, row):\n",
    "                changed = True\n",
    "                self.grid[i, :] = row\n",
    "        \n",
    "        return changed, score_increase\n",
    "    \n",
    "    def _move_right(self):\n",
    "        \"\"\"Move all tiles to the right and merge if possible\"\"\"\n",
    "        score_increase = 0\n",
    "        changed = False\n",
    "        \n",
    "        for i in range(self.size):\n",
    "            original_row = self.grid[i, :].copy()\n",
    "            \n",
    "            # Reverse the row\n",
    "            row = original_row[::-1]\n",
    "            \n",
    "            # Compress (move non-zero elements to the left)\n",
    "            row = self._compress_row(row)\n",
    "            \n",
    "            # Merge\n",
    "            row, score = self._merge_row(row)\n",
    "            score_increase += score\n",
    "            \n",
    "            # Compress again after merging\n",
    "            row = self._compress_row(row)\n",
    "            \n",
    "            # Reverse back\n",
    "            row = row[::-1]\n",
    "            \n",
    "            # Update grid\n",
    "            if not np.array_equal(original_row, row):\n",
    "                changed = True\n",
    "                self.grid[i, :] = row\n",
    "        \n",
    "        return changed, score_increase\n",
    "    \n",
    "    def _move_up(self):\n",
    "        \"\"\"Move all tiles up and merge if possible\"\"\"\n",
    "        score_increase = 0\n",
    "        changed = False\n",
    "        \n",
    "        # Transpose the grid\n",
    "        self.grid = self.grid.T\n",
    "        \n",
    "        # Apply left move logic to each row (which are now columns)\n",
    "        changed, score_increase = self._move_left()\n",
    "        \n",
    "        # Transpose back\n",
    "        self.grid = self.grid.T\n",
    "        \n",
    "        return changed, score_increase\n",
    "    \n",
    "    def _move_down(self):\n",
    "        \"\"\"Move all tiles down and merge if possible\"\"\"\n",
    "        score_increase = 0\n",
    "        changed = False\n",
    "        \n",
    "        # Transpose the grid\n",
    "        self.grid = self.grid.T\n",
    "        \n",
    "        # Apply right move logic to each row (which are now columns)\n",
    "        changed, score_increase = self._move_right()\n",
    "        \n",
    "        # Transpose back\n",
    "        self.grid = self.grid.T\n",
    "        \n",
    "        return changed, score_increase\n",
    "    \n",
    "    def _get_max_tile(self):\n",
    "        \"\"\"Get the value of the highest tile on the board\"\"\"\n",
    "        return np.max(self.grid)\n",
    "    \n",
    "    def _get_state(self):\n",
    "        \"\"\"Return the current state of the game\"\"\"\n",
    "        return self.grid.copy()\n",
    "    \n",
    "    def _is_game_over(self):\n",
    "        \"\"\"Check if the game is over (no empty cells and no possible merges)\"\"\"\n",
    "        # Check for empty cells\n",
    "        if self._has_empty_cells():\n",
    "            return False\n",
    "        \n",
    "        # Check for possible merges horizontally\n",
    "        for i in range(self.size):\n",
    "            for j in range(self.size - 1):\n",
    "                if self.grid[i, j] == self.grid[i, j + 1]:\n",
    "                    return False\n",
    "        \n",
    "        # Check for possible merges vertically\n",
    "        for i in range(self.size - 1):\n",
    "            for j in range(self.size):\n",
    "                if self.grid[i, j] == self.grid[i + 1, j]:\n",
    "                    return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Take an action (0: left, 1: right, 2: up, 3: down) and return new state, reward, done\"\"\"\n",
    "        if self.done:\n",
    "            # If game is already over, return current state with zero reward\n",
    "            return self._get_state(), 0, True, {}\n",
    "        \n",
    "        prev_score = self.score\n",
    "        prev_max_tile = self._get_max_tile()\n",
    "        moved = False\n",
    "        \n",
    "        # Execute the move\n",
    "        if action == 0:  # left\n",
    "            moved, score_increase = self._move_left()\n",
    "        elif action == 1:  # right\n",
    "            moved, score_increase = self._move_right()\n",
    "        elif action == 2:  # up\n",
    "            moved, score_increase = self._move_up()\n",
    "        elif action == 3:  # down\n",
    "            moved, score_increase = self._move_down()\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid action: {action}\")\n",
    "        \n",
    "        # Update score if the move was valid\n",
    "        if moved:\n",
    "            self.score += score_increase\n",
    "            self._add_random_tile()\n",
    "        \n",
    "        # Check if game is over\n",
    "        self.done = self._is_game_over()\n",
    "        \n",
    "        # Calculate reward\n",
    "        current_max_tile = self._get_max_tile()\n",
    "        \n",
    "        # Three components to the reward:\n",
    "        # 1. Points gained from merging tiles\n",
    "        # 2. Bonus for creating a new highest tile\n",
    "        # 3. Penalty for invalid moves\n",
    "        \n",
    "        # Base reward is the score increase\n",
    "        reward = score_increase\n",
    "        \n",
    "        # Bonus for new max tile\n",
    "        if current_max_tile > prev_max_tile:\n",
    "            reward += current_max_tile  # Bonus equal to the value of the new max tile\n",
    "        \n",
    "        # Penalty for invalid moves\n",
    "        if not moved:\n",
    "            reward -= 10  # Small penalty for trying an invalid move\n",
    "        \n",
    "        # Penalty for losing the game\n",
    "        if self.done:\n",
    "            reward -= 50  # Larger penalty for ending the game\n",
    "        \n",
    "        # Return state, reward, done, and info dictionary\n",
    "        info = {\n",
    "            'score': self.score,\n",
    "            'max_tile': current_max_tile,\n",
    "            'moved': moved\n",
    "        }\n",
    "        \n",
    "        return self._get_state(), reward, self.done, info\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"Display the current state of the game\"\"\"\n",
    "        # Convert 0s to empty strings for cleaner display\n",
    "        display_grid = np.where(self.grid > 0, self.grid, '')\n",
    "        \n",
    "        # Print the grid\n",
    "        print(f\"Score: {self.score}\")\n",
    "        print(\"+------+------+------+------+\")\n",
    "        for i in range(self.size):\n",
    "            row_str = \"|\"                    \n",
    "            for j in range(self.size):\n",
    "                cell = display_grid[i, j]\n",
    "                if cell == '':\n",
    "                    row_str += \"      |\"\n",
    "                else:\n",
    "                    row_str += f\" {cell:4d} |\"\n",
    "            print(row_str)\n",
    "            print(\"+------+------+------+------+\")\n",
    "        \n",
    "        if self.done:\n",
    "            print(\"Game Over!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our game environment to make sure it works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the game environment with random moves\n",
    "env = Game2048Env()\n",
    "env.reset()\n",
    "env.render()\n",
    "\n",
    "# Make some random moves\n",
    "for _ in range(10):\n",
    "    action = random.randint(0, 3)\n",
    "    print(f\"\\nAction: {Game2048Env.ACTIONS[action]}\")\n",
    "    state, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    print(f\"Reward: {reward}, Done: {done}, Score: {info['score']}\")\n",
    "    \n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implement Deep Q-Learning Agent\n",
    "\n",
    "Now we'll implement the Deep Q-Learning agent that will learn to play 2048 efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Experience replay buffer to store and sample agent experiences\"\"\"\n",
    "    \n",
    "    def __init__(self, max_size=100000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to the buffer\"\"\"\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a batch of experiences from the buffer\"\"\"\n",
    "        experiences = random.sample(self.buffer, min(batch_size, len(self.buffer)))\n",
    "        \n",
    "        states = np.array([exp[0] for exp in experiences])\n",
    "        actions = np.array([exp[1] for exp in experiences])\n",
    "        rewards = np.array([exp[2] for exp in experiences])\n",
    "        next_states = np.array([exp[3] for exp in experiences])\n",
    "        dones = np.array([exp[4] for exp in experiences])\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def size(self):\n",
    "        \"\"\"Return the current size of the buffer\"\"\"\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_state(state):\n",
    "    \"\"\"Preprocess the game state for neural network input\"\"\"\n",
    "    # Normalize the state by taking log2 of non-zero values (since tiles are powers of 2)\n",
    "    # This helps the neural network learn more efficiently\n",
    "    processed = np.zeros_like(state, dtype=np.float32)\n",
    "    \n",
    "    # For each non-zero tile, take log2 and normalize by dividing by 16 (max tile is usually 2^15 = 32768)\n",
    "    non_zero_mask = state > 0\n",
    "    processed[non_zero_mask] = np.log2(state[non_zero_mask]) / 16.0\n",
    "    \n",
    "    # Reshape to (1, 4, 4, 1) for CNN input or (1, 16) for Dense layers\n",
    "    return processed.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dqn_model(input_shape, num_actions):\n",
    "    \"\"\"Create a deep Q-network model\"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Dense(256, activation='relu', input_shape=input_shape),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(num_actions, activation='linear')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=0.0001), loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \"\"\"Deep Q-Learning agent for playing 2048\"\"\"\n",
    "    \n",
    "    def __init__(self, state_shape, num_actions, gamma=0.99, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.9995, batch_size=64):\n",
    "        # Environment parameters\n",
    "        self.state_shape = (state_shape,)  # Input shape for the model (flattened grid)\n",
    "        self.num_actions = num_actions  # Number of possible actions\n",
    "        \n",
    "        # Hyperparameters\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        self.epsilon_min = epsilon_min  # Minimum exploration rate\n",
    "        self.epsilon_decay = epsilon_decay  # Decay rate for exploration\n",
    "        self.batch_size = batch_size  # Size of batches to sample from replay buffer\n",
    "        \n",
    "        # Create primary and target networks\n",
    "        self.primary_network = create_dqn_model(self.state_shape, self.num_actions)\n",
    "        self.target_network = create_dqn_model(self.state_shape, self.num_actions)\n",
    "        self.update_target_network()\n",
    "        \n",
    "        # Create replay buffer\n",
    "        self.replay_buffer = ReplayBuffer()\n",
    "        \n",
    "        # Metrics for tracking performance\n",
    "        self.loss_history = []\n",
    "        self.reward_history = []\n",
    "        self.max_tile_history = []\n",
    "        self.epsilon_history = []\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        \"\"\"Copy weights from primary network to target network\"\"\"\n",
    "        self.target_network.set_weights(self.primary_network.get_weights())\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        \"\"\"Choose an action using epsilon-greedy policy\"\"\"\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # Explore: choose a random action\n",
    "            return np.random.randint(self.num_actions)\n",
    "        else:\n",
    "            # Exploit: choose the best action according to the policy\n",
    "            q_values = self.primary_network.predict(preprocess_state(state), verbose=0)\n",
    "            return np.argmax(q_values[0])\n",
    "    \n",
    "    def update_replay_buffer(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to the replay buffer\"\"\"\n",
    "        self.replay_buffer.add(state, action, reward, next_state, done)\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Train the agent using experiences from the replay buffer\"\"\"\n",
    "        # Check if we have enough experiences to train\n",
    "        if self.replay_buffer.size() < self.batch_size:\n",
    "            return 0\n",
    "        \n",
    "        # Sample a batch of experiences\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
    "        \n",
    "        # Preprocess states and next_states\n",
    "        processed_states = np.vstack([preprocess_state(state) for state in states])\n",
    "        processed_next_states = np.vstack([preprocess_state(next_state) for next_state in next_states])\n",
    "        \n",
    "        # Get the current Q values from the primary network\n",
    "        current_q_values = self.primary_network.predict(processed_states, verbose=0)\n",
    "        \n",
    "        # Get the next Q values from the target network\n",
    "        next_q_values = self.target_network.predict(processed_next_states, verbose=0)\n",
    "        \n",
    "        # Initialize the target Q values as the current Q values (we'll update only the chosen actions)\n",
    "        target_q_values = current_q_values.copy()\n",
    "        \n",
    "        # Update the Q values for the actions taken\n",
    "        for i in range(len(actions)):\n",
    "            if dones[i]:\n",
    "                # If the episode ended, there is no next Q value\n",
    "                target_q_values[i, actions[i]] = rewards[i]\n",
    "            else:\n",
    "                # Otherwise, use the Bellman equation to compute the target Q value\n",
    "                target_q_values[i, actions[i]] = rewards[i] + self.gamma * np.max(next_q_values[i])\n",
    "        \n",
    "        # Train the primary network\n",
    "        loss = self.primary_network.train_on_batch(processed_states, target_q_values)\n",
    "        \n",
    "        # Decay epsilon\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def save_model(self, filepath):\n",
    "        \"\"\"Save the model weights\"\"\"\n",
    "        self.primary_network.save_weights(filepath)\n",
    "    \n",
    "    def load_model(self, filepath):\n",
    "        \"\"\"Load the model weights\"\"\"\n",
    "        self.primary_network.load_weights(filepath)\n",
    "        self.update_target_network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training the Agent\n",
    "\n",
    "Now we'll train our DQN agent to play 2048."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(agent, env, num_episodes=10000, target_update_freq=10, render_freq=1000, save_freq=100):\n",
    "    \"\"\"Train the agent for a specified number of episodes\"\"\"\n",
    "    max_score = 0\n",
    "    max_tile = 0\n",
    "    episode_scores = []\n",
    "    episode_max_tiles = []\n",
    "    \n",
    "    # Set up directories for saving models\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        steps = 0\n",
    "        \n",
    "        while not done:\n",
    "            # Choose an action\n",
    "            action = agent.get_action(state)\n",
    "            \n",
    "            # Take a step in the environment\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            # Store the experience in replay buffer\n",
    "            agent.update_replay_buffer(state, action, reward, next_state, done)\n",
    "            \n",
    "            # Train the agent\n",
    "            loss = agent.train()\n",
    "            if loss > 0:\n",
    "                agent.loss_history.append(loss)\n",
    "            \n",
    "            # Move to the next state\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            # Update the target network periodically\n",
    "            if steps % target_update_freq == 0:\n",
    "                agent.update_target_network()\n",
    "        \n",
    "        # Store episode results\n",
    "        episode_scores.append(info['score'])\n",
    "        episode_max_tiles.append(info['max_tile'])\n",
    "        \n",
    "        # Update max score and max tile\n",
    "        if info['score'] > max_score:\n",
    "            max_score = info['score']\n",
    "        if info['max_tile'] > max_tile:\n",
    "            max_tile = info['max_tile']\n",
    "        \n",
    "        # Track epsilon\n",
    "        agent.epsilon_history.append(agent.epsilon)\n",
    "        \n",
    "        # Render occasionally to see progress\n",
    "        if (episode + 1) % render_freq == 0 or (episode + 1) == num_episodes:\n",
    "            print(f\"\\nEpisode {episode + 1}/{num_episodes}\")\n",
    "            print(f\"Score: {info['score']}, Max Tile: {info['max_tile']}, Epsilon: {agent.epsilon:.4f}\")\n",
    "            print(f\"Max Score so far: {max_score}, Max Tile so far: {max_tile}\")\n",
    "            env.render()\n",
    "        \n",
    "        # Save the model periodically\n",
    "        if (episode + 1) % save_freq == 0:\n",
    "            agent.save_model(f\"models/dqn_2048_episode_{episode + 1}.h5\")\n",
    "    \n",
    "    # Save final model\n",
    "    agent.save_model(\"models/dqn_2048_final.h5\")\n",
    "    \n",
    "    return episode_scores, episode_max_tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize environment and agent\n",
    "env = Game2048Env()\n",
    "state_shape = 16  # 4x4 grid flattened\n",
    "num_actions = 4   # left, right, up, down\n",
    "\n",
    "# Create agent with specified hyperparameters\n",
    "agent = DQNAgent(\n",
    "    state_shape=state_shape,\n",
    "    num_actions=num_actions,\n",
    "    gamma=0.99,          # Discount factor\n",
    "    epsilon=1.0,         # Initial exploration rate\n",
    "    epsilon_min=0.01,    # Minimum exploration rate\n",
    "    epsilon_decay=0.9995, # Decay rate for exploration\n",
    "    batch_size=64        # Batch size for training\n",
    ")\n",
    "\n",
    "# Train the agent\n",
    "# Reduce num_episodes for faster execution (e.g., 1000 for testing)\n",
    "print(\"Starting training...\")\n",
    "episode_scores, episode_max_tiles = train_agent(\n",
    "    agent=agent,\n",
    "    env=env,\n",
    "    num_episodes=5000,    # Number of episodes to train for\n",
    "    target_update_freq=10, # How often to update the target network\n",
    "    render_freq=500,      # How often to render the game\n",
    "    save_freq=1000        # How often to save the model\n",
    ")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualizing Agent Performance\n",
    "\n",
    "Let's visualize how our agent performed during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_results(agent, episode_scores, episode_max_tiles):\n",
    "    \"\"\"Plot the training results\"\"\"\n",
    "    # Create a figure with multiple subplots\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Plot the score over episodes\n",
    "    axs[0, 0].plot(episode_scores)\n",
    "    axs[0, 0].set_title('Score per Episode')\n",
    "    axs[0, 0].set_xlabel('Episode')\n",
    "    axs[0, 0].set_ylabel('Score')\n",
    "    \n",
    "    # Plot the max tile over episodes\n",
    "    axs[0, 1].plot(episode_max_tiles)\n",
    "    axs[0, 1].set_title('Max Tile per Episode')\n",
    "    axs[0, 1].set_xlabel('Episode')\n",
    "    axs[0, 1].set_ylabel('Max Tile')\n",
    "    \n",
    "    # Plot the loss over training steps\n",
    "    if len(agent.loss_history) > 0:\n",
    "        axs[1, 0].plot(agent.loss_history)\n",
    "        axs[1, 0].set_title('Loss over Training Steps')\n",
    "        axs[1, 0].set_xlabel('Training Step')\n",
    "        axs[1, 0].set_ylabel('Loss')\n",
    "    \n",
    "    # Plot the epsilon over episodes\n",
    "    axs[1, 1].plot(agent.epsilon_history)\n",
    "    axs[1, 1].set_title('Epsilon over Episodes')\n",
    "    axs[1, 1].set_xlabel('Episode')\n",
    "    axs[1, 1].set_ylabel('Epsilon')\n",
    "    \n",
    "    # Calculate moving averages for smoother plots\n",
    "    window_size = min(100, len(episode_scores))\n",
    "    if window_size > 0 and len(episode_scores) > window_size:\n",
    "        scores_avg = np.convolve(episode_scores, np.ones(window_size)/window_size, mode='valid')\n",
    "        axs[0, 0].plot(range(window_size-1, len(scores_avg) + window_size-1), scores_avg, 'r-', label=f'{window_size}-episode avg')\n",
    "        axs[0, 0].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print some statistics\n",
    "    print(f\"Training Statistics:\")\n",
    "    print(f\"Number of Episodes: {len(episode_scores)}\")\n",
    "    print(f\"Final Epsilon: {agent.epsilon:.4f}\")\n",
    "    print(f\"Max Score: {max(episode_scores)}\")\n",
    "    print(f\"Max Tile: {max(episode_max_tiles)}\")\n",
    "    print(f\"Average Score (last 100 episodes): {np.mean(episode_scores[-100:]):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training results\n",
    "plot_training_results(agent, episode_scores, episode_max_tiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluating the Trained Agent\n",
    "\n",
    "Now let's evaluate our trained agent by having it play a few games without exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(agent, env, num_episodes=10, render=True):\n",
    "    \"\"\"Evaluate the agent's performance with no exploration\"\"\"\n",
    "    # Temporarily set epsilon to 0 for evaluation\n",
    "    original_epsilon = agent.epsilon\n",
    "    agent.epsilon = 0.0\n",
    "    \n",
    "    scores = []\n",
    "    max_tiles = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        if render:\n",
    "            print(f\"\\nEvaluation Episode {episode + 1}/{num_episodes}\")\n",
    "            env.render()\n",
    "        \n",
    "        while not done:\n",
    "            # Choose the best action according to the learned policy\n",
    "            action = agent.get_action(state)  # With epsilon=0, this will always choose the best action\n",
    "            \n",
    "            # Take the action\n",
    "            state, _, done, info = env.step(action)\n",
    "            \n",
    "            if render:\n",
    "                print(f\"\\nAction: {Game2048Env.ACTIONS[action]}\")\n",
    "                env.render()\n",
    "        \n",
    "        scores.append(info['score'])\n",
    "        max_tiles.append(info['max_tile'])\n",
    "        \n",
    "        if render:\n",
    "            print(f\"Final Score: {info['score']}, Max Tile: {info['max_tile']}\")\n",
    "    \n",
    "    # Restore original epsilon\n",
    "    agent.epsilon = original_epsilon\n",
    "    \n",
    "    # Print evaluation results\n",
    "    print(f\"\\nEvaluation Results:\")\n",
    "    print(f\"Average Score: {np.mean(scores):.2f}\")\n",
    "    print(f\"Max Score: {max(scores)}\")\n",
    "    print(f\"Average Max Tile: {np.mean(max_tiles):.2f}\")\n",
    "    print(f\"Max Tile Achieved: {max(max_tiles)}\")\n",
    "    \n",
    "    return scores, max_tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained agent\n",
    "print(\"Evaluating the trained agent...\")\n",
    "eval_scores, eval_max_tiles = evaluate_agent(agent, env, num_episodes=3, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Improving the Agent\n",
    "\n",
    "Here are some ideas for further improving the agent:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Better State Representation**: Use a CNN to capture the spatial structure of the grid.\n",
    "2. **Improved Reward Function**: Design a more sophisticated reward function that considers board structure.\n",
    "3. **Additional Training**: Continue training for more episodes to improve performance.\n",
    "4. **Advanced Techniques**: Implement extensions like Double DQN or Dueling DQN.\n",
    "5. **Hyperparameter Tuning**: Experiment with different hyperparameters to optimize performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "In this notebook, we've implemented a Deep Q-Learning agent to play the 2048 game. The agent learns to maximize its score by making strategic moves based on the current game state.\n",
    "\n",
    "The training process involves:\n",
    "1. Creating a 2048 game environment suitable for reinforcement learning\n",
    "2. Implementing a Deep Q-Network with experience replay\n",
    "3. Training the agent using epsilon-greedy exploration\n",
    "4. Evaluating the trained agent's performance\n",
    "\n",
    "While our implementation achieves decent results, there's always room for improvement with more sophisticated techniques and longer training periods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
